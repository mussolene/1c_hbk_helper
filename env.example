# 1C Help — переменные окружения
# Скопируйте в .env и задайте значения под своё окружение.
# Не коммитьте .env в репозиторий.
#
# Для Docker Compose: значения ниже — база для .env; docker-compose задаёт свои defaults
# (EMBEDDING_BACKEND, MCP_TRANSPORT и др.). Для локального запуска — раскомментируйте и задайте нужное.

# --- Qdrant ---
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=onec_help
# Путь к каталогу хранилища Qdrant (для index-status: вывод размера БД на диске). Например ./data/qdrant
# QDRANT_STORAGE_PATH=./data/qdrant

# --- Пути к справке ---
# Базовый каталог для MCP и serve (где лежат распакованные .md/.html)
HELP_PATH=/data

# Корень каталогов с версиями 1С для ingest (подпапки = версии, внутри — .hbk)
HELP_SOURCE_BASE=/opt/1cv8
# Альтернативное имя того же
# HELP_SOURCES_DIR=/opt/1cv8

# Список путей через запятую (если не используете HELP_SOURCE_BASE)
# HELP_SOURCE_DIRS=/path/8.3.27,/path/8.3.26

# Языки справки для ingest (ru, en и т.д.)
HELP_LANGUAGES=ru

# Кодировка при чтении файлов справки: по умолчанию UTF-8, затем CP1251. Если вся справка в 1251 — задайте cp1251
# HELP_FILE_ENCODING=

# Временный каталог для ingest. Если не задан — используется $TMPDIR/help_ingest или tempfile.gettempdir()
# HELP_INGEST_TEMP=/tmp/help_ingest

# Кэш ingest (SQLite): по хэшу .hbk пропуск уже проиндексированных (не парсить, не делать embedding повторно).
# В Docker задайте путь на постоянном томе, например /qdrant_storage/ingest_cache.db
# INGEST_CACHE_FILE=/tmp/onec_help_ingest_cache.db
# Полная переиндексация без кэша: INGEST_SKIP_CACHE=1 или ingest --no-cache

# --- Эмбеддинги (ingest и поиск в MCP) ---
# Бэкенд: local (sentence-transformers), openai_api (LM Studio, Ollama и т.п.), deterministic (384-dim без модели — только БД) или none (плейсхолдер, только keyword-поиск)
EMBEDDING_BACKEND=openai_api

# Имя модели: для local — all-MiniLM-L6-v2 (HuggingFace); для openai_api — желаемая модель. Если её нет на сервере (LM Studio), берётся первая из списка или предпочтительная
EMBEDDING_MODEL=text-text-embedding-mxbai-embed-large-v1

# Для openai_api: базовый URL. По умолчанию LM Studio на хосте: localhost:1234 (локально) или host.docker.internal:1234 (в контейнере)
# EMBEDDING_API_URL=http://host.docker.internal:1234/v1
# Ключ API (если нужен)
# EMBEDDING_API_KEY=
# Размерность вектора при openai_api (обязательно задать под вашу модель, иначе 384)
# EMBEDDING_DIMENSION=768

# Размер батча для эмбеддингов (текстов за один вызов encode/API). Больше — быстрее при local, осторожнее при API.
# EMBEDDING_BATCH_SIZE=64
# Число параллельных запросов к внешнему API эмбеддингов (только openai_api). Увеличивает нагрузку на сервис.
# EMBEDDING_WORKERS=4
# Максимальная мощность: 1/true/yes — использовать макс. батч (256) и макс. воркеры (16) для любого типа embedding.
# EMBEDDING_FORCE_BATCH=0
# Макс. число одновременных запросов к API эмбеддингов (при ingest с несколькими воркерами — снижает перегрузку LM Studio).
# EMBEDDING_MAX_CONCURRENT=8
# Таймаут HTTP-запроса к API эмбеддингов (секунды). При недоступности/таймауте — retry и затем плейсхолдер.
# EMBEDDING_TIMEOUT=60
# Таймаут для batch-запроса (секунды). По умолчанию — max(EMBEDDING_TIMEOUT, 30 + batch_size/10).
# EMBEDDING_BATCH_TIMEOUT=120

# Токен HuggingFace (опционально): убирает предупреждение при загрузке локальной модели
# HF_TOKEN=hf_...

# Файл для списка неудачных .hbk при ingest (опционально)
# INGEST_FAILED_LOG=/var/log/ingest_failed.log

# Файл статуса ingest (index-status: скорость эмбеддингов, текущий файл по потокам, прогресс по папкам, ETA)
# INDEX_STATUS_FILE=/tmp/onec_help_ingest_status.json
# Как часто обновлять файл статуса (секунды). По умолчанию 2.
# INDEX_STATUS_INTERVAL_SEC=2

# --- Веб (serve) ---
PORT=5000
# Хост: 127.0.0.1 (только localhost) или 0.0.0.0 (Docker, доступ из сети)
# HELP_SERVE_HOST=127.0.0.1

# Список разрешённых базовых каталогов (через запятую). Обязательно для serve: без него форма и CLI не принимают пути.
# HELP_SERVE_ALLOWED_DIRS=/opt/1cv8,/opt/help

# Отключить debug даже при --debug (безопасность в проде)
# PRODUCTION=1

# --- Сниппеты (load-snippets, parse-fastcode) ---
# docs/snippets/ — примеры, не загружаются.
# Docker: -v /path/to/snippets:/data/snippets и SNIPPETS_DIR=/data/snippets
# parse-fastcode пишет в SNIPPETS_DIR/fastcode_snippets.json (требуется rw)
# SAVE_SNIPPET_TO_FILES=1 — save_1c_snippet также пишет в SNIPPETS_DIR (.md с frontmatter)
# SNIPPETS_DIR=/data/snippets
# SNIPPETS_HOST_PATH=./snippets   # путь на хосте для docker-compose

# --- Стандарты (load-standards) ---
# По умолчанию загружаются совместно оба репо. Формат: owner/repo или owner/repo:branch
# STANDARDS_REPOS=1C-Company/v8-code-style:master,zeegin/v8std:main
# STANDARDS_SUBPATH=docs
# Вариант 2: смонтированная папка (как сниппеты). Docker: -v ./standards:/data/standards
# STANDARDS_DIR=/data/standards
# STANDARDS_HOST_PATH=./standards

# --- Память (MCP, при MEMORY_ENABLED=1) ---
# MEMORY_ENABLED=0
# MEMORY_BASE_PATH=~/.onec_help
# MEMORY_SHORT_LIMIT=50
# MEMORY_MEDIUM_LIMIT=500
# MEMORY_MEDIUM_TTL_DAYS=7

# --- Watchdog (мониторинг .hbk, pending memory) ---
# WATCHDOG_ENABLED=0
# WATCHDOG_POLL_INTERVAL=600
# WATCHDOG_PENDING_INTERVAL=600

# --- BSL LS (bsl-bridge) ---
# Проект в volume .:/projects. BSL_WORKSPACE_ROOT — путь к конфигу внутри контейнера (например /projects или /projects/src)
# BSL_HOST_PROJECTS_ROOT — подставляется через make up (pwd)
# BSL_WORKSPACE_ROOT=/projects
# BSL_CONTAINER_MEMORY=4g
# BSL_LS_VERSION=latest
# MCP_LSP_BSL_JAVA_XMX=2g
# MCP_LSP_BSL_JAVA_XMS=512m

# --- MCP ---
# Транспорт: stdio (локально), http или streamable-http (в Docker, для Cursor рекомендуется streamable-http)
# По умолчанию: streamable-http — для удалённого доступа (Docker, Cursor по URL)
MCP_TRANSPORT=streamable-http
MCP_HOST=127.0.0.1
MCP_PORT=5050
MCP_PATH=/mcp
# Лимит запросов в минуту (120 по умолчанию). 0 — отключить
# MCP_RATE_LIMIT_PER_MIN=120
