# 1C Help — переменные окружения
# Скопируйте в .env и задайте значения под своё окружение.
# Не коммитьте .env в репозиторий.

# --- Qdrant ---
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=onec_help
# Путь к каталогу хранилища Qdrant (для index-status: вывод размера БД на диске). Например ./data/qdrant
# QDRANT_STORAGE_PATH=./data/qdrant

# --- Пути к справке ---
# Базовый каталог для MCP и serve (где лежат распакованные .md/.html)
HELP_PATH=/data

# Корень каталогов с версиями 1С для ingest (подпапки = версии, внутри — .hbk)
HELP_SOURCE_BASE=/opt/1cv8
# Альтернативное имя того же
# HELP_SOURCES_DIR=/opt/1cv8

# Список путей через запятую (если не используете HELP_SOURCE_BASE)
# HELP_SOURCE_DIRS=/path/8.3.27,/path/8.3.26

# Языки справки для ingest (ru, en и т.д.)
HELP_LANGUAGES=ru

# Кодировка при чтении файлов справки: по умолчанию UTF-8, затем CP1251. Если вся справка в 1251 — задайте cp1251
# HELP_FILE_ENCODING=

# Временный каталог для ingest (распаковка и сборка перед индексацией)
HELP_INGEST_TEMP=/tmp/help_ingest

# Кэш ingest (SQLite): по хэшу .hbk пропуск уже проиндексированных (не парсить, не делать embedding повторно).
# В Docker задайте путь на постоянном томе, например /qdrant_storage/ingest_cache.db
# INGEST_CACHE_FILE=/tmp/onec_help_ingest_cache.db
# Полная переиндексация без кэша: INGEST_SKIP_CACHE=1 или ingest --no-cache

# --- Эмбеддинги (ingest и поиск в MCP) ---
# Бэкенд: local (sentence-transformers), openai_api (LM Studio, Ollama, llama.cpp и т.п.) или none (отключены — только плейсхолдер, семантический поиск не работает, search_1c_help_keyword работает)
# EMBEDDING_BACKEND=local

# Имя модели: для local — модель с HuggingFace; для openai_api — желаемая модель. Если её нет на сервере (LM Studio), берётся первая из списка или предпочтительная (nomic-embed-text, all-MiniLM-L6-v2 и т.д.), при необходимости — запрос на загрузку через API LM Studio
# EMBEDDING_MODEL=all-MiniLM-L6-v2

# Для openai_api: базовый URL. По умолчанию LM Studio на хосте: localhost:1234 (локально) или host.docker.internal:1234 (в контейнере)
# EMBEDDING_API_URL=http://host.docker.internal:1234/v1
# Ключ API (если нужен)
# EMBEDDING_API_KEY=
# Размерность вектора при openai_api (обязательно задать под вашу модель, иначе 384)
# EMBEDDING_DIMENSION=768

# Размер батча для эмбеддингов (текстов за один вызов encode/API). Больше — быстрее при local, осторожнее при API.
# EMBEDDING_BATCH_SIZE=64
# Число параллельных запросов к внешнему API эмбеддингов (только openai_api). Увеличивает нагрузку на сервис.
# EMBEDDING_WORKERS=4
# Максимальная мощность: 1/true/yes — использовать макс. батч (256) и макс. воркеры (16) для любого типа embedding.
# EMBEDDING_FORCE_BATCH=0
# Таймаут HTTP-запроса к API эмбеддингов (секунды). При недоступности/таймауте — retry и затем плейсхолдер.
# EMBEDDING_TIMEOUT=60

# Токен HuggingFace (опционально): убирает предупреждение при загрузке локальной модели
# HF_TOKEN=hf_...

# Файл для списка неудачных .hbk при ingest (опционально)
# INGEST_FAILED_LOG=/var/log/ingest_failed.log

# Файл статуса ingest (index-status: скорость эмбеддингов, текущий файл по потокам, прогресс по папкам, ETA)
# INDEX_STATUS_FILE=/tmp/onec_help_ingest_status.json
# Как часто обновлять файл статуса (секунды). По умолчанию 2.
# INDEX_STATUS_INTERVAL_SEC=2

# --- Веб (serve) ---
PORT=5000

# Список разрешённых базовых каталогов (через запятую). Если задан — форма принимает только пути из списка.
# HELP_SERVE_ALLOWED_DIRS=/opt/1cv8,/opt/help

# Отключить debug даже при --debug (безопасность в проде)
# PRODUCTION=1

# --- Память (MCP, при MEMORY_ENABLED=1) ---
# MEMORY_ENABLED=0
# MEMORY_BASE_PATH=~/.onec_help
# MEMORY_SHORT_LIMIT=50
# MEMORY_MEDIUM_LIMIT=500
# MEMORY_MEDIUM_TTL_DAYS=7

# --- Watchdog (мониторинг .hbk, pending memory) ---
# WATCHDOG_ENABLED=0
# WATCHDOG_POLL_INTERVAL=600
# WATCHDOG_PENDING_INTERVAL=600

# --- MCP ---
# Транспорт: stdio или http
MCP_TRANSPORT=stdio
MCP_HOST=127.0.0.1
MCP_PORT=5050
MCP_PATH=/mcp
